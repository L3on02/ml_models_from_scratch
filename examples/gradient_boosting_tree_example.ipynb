{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Tree\n",
    "\n",
    "For the final example we turn to the gradient boosting tree. As in the random forrest notebook, we limit our selection of datasets to those the standalone decision tree struggled with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "from models.gradient_boosting_tree import GradientBoostingClassifier as OwnGradientBoostingClassifier, GradientBoostingRegressor as OwnGradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier as SklearnGradientBoostingClassifier, GradientBoostingRegressor as SklearnGradientBoostingRegressor\n",
    "\n",
    "from utils.reports import evaluate_classification, evaluate_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_diabetes\n",
    "from datasets.diamonds import load_diamonds\n",
    "\n",
    "ds_c_hard = load_digits()\n",
    "X, Y = ds_c_hard.data, ds_c_hard.target\n",
    "X_c_hard_train, X_c_hard_test, Y_c_hard_train, Y_c_hard_test = train_test_split(X , Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_r_medium = load_diabetes()\n",
    "X, Y = ds_r_medium.data, ds_r_medium.target\n",
    "X_r_medium_train, X_r_medium_test, Y_r_medium_train, Y_r_medium_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_r_hard = load_diamonds()\n",
    "X, Y = ds_r_hard.data, ds_r_hard.target\n",
    "X_r_hard_train, X_r_hard_test, Y_r_hard_train, Y_r_hard_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the digits dataset, we first test the classification capabilities of our model. We again refrain from doing any hyperparameter tuning for the sake of performance when running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.87, Recall: 0.87, F1-Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "gbt_classifier = OwnGradientBoostingClassifier()\n",
    "\n",
    "gbt_classifier.fit(X_c_hard_train, Y_c_hard_train)\n",
    "Y_c_hard_pred = gbt_classifier.predict(X_c_hard_test)\n",
    "\n",
    "evaluate_classification(Y_c_hard_test, Y_c_hard_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.97, Recall: 0.97, F1-Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "gbt_classifier = SklearnGradientBoostingClassifier()\n",
    "\n",
    "gbt_classifier.fit(X_c_hard_train, Y_c_hard_train)\n",
    "Y_c_hard_pred = gbt_classifier.predict(X_c_hard_test)\n",
    "\n",
    "evaluate_classification(Y_c_hard_test, Y_c_hard_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the digits dataset, we start to notice that missing optimizations in our implementation are starting to become a problem. We train an individual GBTR on each of the 10 classes which results in a problematic amount of training time. So much as that we are limited in number of iterations we can run, leading to the first real divergence in accuracy between our implementation and that of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 41.26, MSE: 2549.36, R²: 0.52\n"
     ]
    }
   ],
   "source": [
    "gbt_regressor = OwnGradientBoostingRegressor()\n",
    "\n",
    "gbt_regressor.fit(X_r_medium_train, Y_r_medium_train)\n",
    "Y_r_medium_pred = gbt_regressor.predict(X_r_medium_test)\n",
    "\n",
    "evaluate_regression(Y_r_medium_test, Y_r_medium_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 44.67, MSE: 2913.18, R²: 0.45\n"
     ]
    }
   ],
   "source": [
    "gbt_regressor = SklearnGradientBoostingRegressor()\n",
    "\n",
    "gbt_regressor.fit(X_r_medium_train, Y_r_medium_train)\n",
    "Y_r_medium_pred = gbt_regressor.predict(X_r_medium_test)\n",
    "\n",
    "evaluate_regression(Y_r_medium_test, Y_r_medium_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly our GBT implementation is able to outperform the sklearn implementation on the diabetes dataset. This is likely due to the fact that the dataset is rather small, thus the overhead of our implementation is less of a problem and we are not limited in the number of iterations we can run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 270.51, MSE: 298296.96, R²: 0.98\n"
     ]
    }
   ],
   "source": [
    "gbt_regressor = OwnGradientBoostingRegressor()\n",
    "\n",
    "gbt_regressor.fit(X_r_hard_train, Y_r_hard_train)\n",
    "Y_r_hard_pred = gbt_regressor.predict(X_r_hard_test)\n",
    "\n",
    "evaluate_regression(Y_r_hard_test, Y_r_hard_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 336.51, MSE: 371056.33, R²: 0.98\n"
     ]
    }
   ],
   "source": [
    "gbt_regressor = SklearnGradientBoostingRegressor()\n",
    "\n",
    "gbt_regressor.fit(X_r_hard_train, Y_r_hard_train)\n",
    "Y_r_hard_pred = gbt_regressor.predict(X_r_hard_test)\n",
    "\n",
    "evaluate_regression(Y_r_hard_test, Y_r_hard_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Diamonds dataset, both implementations are able to achieve a similar r2 score with our implementation even boasting a slightly lower mean absolute error. However the size if the dataset highlights the inefficiencies in our implementation once again, as the training time is almost 50 times longer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
