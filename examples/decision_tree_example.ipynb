{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "In this example, we will take a look at the Decision Tree and test it's performance on several datasets while comparing it to the performance of scikit-learn's Decision Tree on the same datasets. The datasets used for testing are 5 in total, 3 for classification and 2 for regression with increasing complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the necessary datasets and split them into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "from models.decision_tree import DecisionTreeClassifier as OwnDecisionTreeClassifier, DecisionTreeRegressor as OwnDecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDecisionTreeClassifier, DecisionTreeRegressor as SklearnDecisionTreeRegressor\n",
    "\n",
    "from utils.reports import evaluate_classification, evaluate_regression\n",
    "from utils.grid_search_cv import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "\n",
    "# set up a list of hyperparameters to search over for all trees\n",
    "params = {\n",
    "    'max_depth': [3, 5, 7, 11],\n",
    "    'min_samples_split': [2, 5, 7],\n",
    "    'min_samples_leaf': [2, 5, 7]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "\n",
    "# Load datasets\n",
    "# one easy, one medium, one hard for each classification and regression\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits\n",
    "from sklearn.datasets import load_diabetes, fetch_california_housing\n",
    "\n",
    "# diamonds dataset is a very large regression dataset which will test the efficiency of the algorithms\n",
    "from datasets.diamonds import load_diamonds\n",
    "\n",
    "\n",
    "ds_c_easy = load_iris()\n",
    "X, Y = ds_c_easy.data, ds_c_easy.target\n",
    "X_c_easy_train, X_c_easy_test, Y_c_easy_train, Y_c_easy_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_c_medium = load_breast_cancer()\n",
    "X, Y = ds_c_medium.data, ds_c_medium.target \n",
    "X_c_medium_train, X_c_medium_test, Y_c_medium_train, Y_c_medium_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_c_hard = load_digits()\n",
    "X, Y = ds_c_hard.data, ds_c_hard.target\n",
    "X_c_hard_train, X_c_hard_test, Y_c_hard_train, Y_c_hard_test = train_test_split(X , Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_r_easy = fetch_california_housing()\n",
    "X, Y = ds_r_easy.data, ds_r_easy.target\n",
    "X_r_easy_train, X_r_easy_test, Y_r_easy_train, Y_r_easy_test = train_test_split(X , Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_r_medium = load_diabetes()\n",
    "X, Y = ds_r_medium.data, ds_r_medium.target\n",
    "X_r_medium_train, X_r_medium_test, Y_r_medium_train, Y_r_medium_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_r_hard = load_diamonds()\n",
    "X, Y = ds_r_hard.data, ds_r_hard.target\n",
    "X_r_hard_train, X_r_hard_test, Y_r_hard_train, Y_r_hard_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "First we will look at our own implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.00, Recall: 1.00, F1-Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = OwnDecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "dt_classifier.fit(X_c_easy_train, Y_c_easy_train)\n",
    "Y_c_easy_pred = dt_classifier.predict(X_c_easy_test)\n",
    "\n",
    "evaluate_classification(Y_c_easy_test, Y_c_easy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model achieves a perfect accuracy on the Iris dataset (This is to be expected as the dataset is very simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.00, Recall: 1.00, F1-Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = SklearnDecisionTreeClassifier(max_depth=5)\n",
    "dt_classifier.fit(X_c_easy_train, Y_c_easy_train)\n",
    "Y_c_easy_pred = dt_classifier.predict(X_c_easy_test)\n",
    "\n",
    "evaluate_classification(Y_c_easy_test, Y_c_easy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without much surprise, scikit-learn's Decision Tree also achieves a perfect accuracy here. Moving on the Breast Cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.96, Recall: 1.00, F1-Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = OwnDecisionTreeClassifier()\n",
    "\n",
    "dt_classifier.fit(X_c_medium_train, Y_c_medium_train)\n",
    "Y_c_medium_pred = dt_classifier.predict(X_c_medium_test)\n",
    "\n",
    "evaluate_classification(Y_c_medium_test, Y_c_medium_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.96, Recall: 0.96, F1-Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = SklearnDecisionTreeClassifier()\n",
    "\n",
    "dt_classifier.fit(X_c_medium_train, Y_c_medium_train)\n",
    "Y_c_medium_pred = dt_classifier.predict(X_c_medium_test)\n",
    "\n",
    "evaluate_classification(Y_c_medium_test, Y_c_medium_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, while not completely perfect, both models still achieve a very high, similar accuracy of 96% with our model even slightly edgeing out scikits implementation in terms of recall. Finally, the digits dataset. This is particularly tricky since we are now dealing with non-binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.88, Recall: 0.87, F1-Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = OwnDecisionTreeClassifier()\n",
    "\n",
    "dt_classifier.fit(X_c_hard_train, Y_c_hard_train)\n",
    "Y_c_hard_pred = dt_classifier.predict(X_c_hard_test)\n",
    "\n",
    "evaluate_classification(Y_c_hard_test, Y_c_hard_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.85, Recall: 0.84, F1-Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = SklearnDecisionTreeClassifier()\n",
    "\n",
    "dt_classifier.fit(X_c_hard_train, Y_c_hard_train)\n",
    "Y_c_hard_pred = dt_classifier.predict(X_c_hard_test)\n",
    "\n",
    "evaluate_classification(Y_c_hard_test, Y_c_hard_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately can see, that both models take a considerable hit in accuracy. Given the complexity of the dataset, this is to be expected. However, the de-facto standard implementation of a decision tree by sklearn only actually performs (marginally) worse. This indicates that the issue doesn't lie in our implementation, but rather that we are reaching the limits of what single a decision tree can achieve on this dataset. After taking a look at the regression datasets, we  will see how our ensamble methods can help us improve on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor\n",
    "We will start with a comparatively simple dataset, the diabetes dataset which aims to predict the progression of diabetes one year after baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.39, MSE: 0.36, R²: 0.73\n"
     ]
    }
   ],
   "source": [
    "dt_regressor = OwnDecisionTreeRegressor()\n",
    "\n",
    "dt_regressor.fit(X_r_easy_train, Y_r_easy_train)\n",
    "Y_r_easy_pred = dt_regressor.predict(X_r_easy_test)\n",
    "\n",
    "evaluate_regression(Y_r_easy_test, Y_r_easy_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.45, MSE: 0.49, R²: 0.62\n"
     ]
    }
   ],
   "source": [
    "dt_regressor = SklearnDecisionTreeRegressor()\n",
    "\n",
    "dt_regressor.fit(X_r_easy_train, Y_r_easy_train)\n",
    "Y_r_easy_pred = dt_regressor.predict(X_r_easy_test)\n",
    "\n",
    "evaluate_regression(Y_r_easy_test, Y_r_easy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our own model outperforms the scikit implementation, however this is due to our models default hyperparameters being better suited for this particular dataset. For the following examples, we will use a grid search accross a range of parameters combined with cross-validation to find the best hyperparameters for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are: {'max_depth': 3, 'min_samples_leaf': 7, 'min_samples_split': 2}\n",
      "MAE: 43.71, MSE: 3038.40, R²: 0.43\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(OwnDecisionTreeRegressor, param_grid) # param_grid is defined above\n",
    "\n",
    "grid_search.fit(X_r_medium_train, Y_r_medium_train)\n",
    "print(f\"The best hyperparameters are: {grid_search.best_params}\")\n",
    "\n",
    "Y_r_medium_pred = grid_search.predict(X_r_medium_test)\n",
    "evaluate_regression(Y_r_medium_test, Y_r_medium_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are: {'max_depth': 3, 'min_samples_leaf': 7, 'min_samples_split': 2}\n",
      "MAE: 45.18, MSE: 3094.50, R²: 0.42\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(SklearnDecisionTreeRegressor, param_grid) # param_grid is defined above\n",
    "\n",
    "grid_search.fit(X_r_medium_train, Y_r_medium_train)\n",
    "print(f\"The best hyperparameters are: {grid_search.best_params}\")\n",
    "\n",
    "Y_r_medium_pred = grid_search.predict(X_r_medium_test)\n",
    "evaluate_regression(Y_r_medium_test, Y_r_medium_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that both models again are very close in terms of results, however an r2 score of 0.4 is not very good, indicating that we are again reaching the limits of what a single decision tree can achieve on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are: {'max_depth': 11, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "MAE: 321.93, MSE: 364038.26, R²: 0.98\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(OwnDecisionTreeRegressor, param_grid)\n",
    "\n",
    "grid_search.fit(X_r_hard_train, Y_r_hard_train)\n",
    "print(f\"The best hyperparameters are: {grid_search.best_params}\")\n",
    "\n",
    "Y_r_hard_pred = grid_search.predict(X_r_hard_test)\n",
    "evaluate_regression(Y_r_hard_test, Y_r_hard_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are: {'max_depth': 11, 'min_samples_leaf': 7, 'min_samples_split': 2}\n",
      "MAE: 319.39, MSE: 374653.63, R²: 0.98\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(SklearnDecisionTreeRegressor, param_grid)\n",
    "\n",
    "grid_search.fit(X_r_hard_train, Y_r_hard_train)\n",
    "print(f\"The best hyperparameters are: {grid_search.best_params}\")\n",
    "\n",
    "Y_r_hard_pred = grid_search.predict(X_r_hard_test)\n",
    "evaluate_regression(Y_r_hard_test, Y_r_hard_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
