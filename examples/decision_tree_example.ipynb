{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "In this example, we will take a look at the Decision Tree and test it's performance on several datasets while comparing it to the performance of scikit-learn's Decision Tree on the same datasets. The datasets used for testing are 5 in total, 3 for classification and 2 for regression with increasing complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "from models.decision_tree import DecisionTreeClassifier as OwnDecisionTreeClassifier, DecisionTreeRegressor as OwnDecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDecisionTreeClassifier, DecisionTreeRegressor as SklearnDecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the necessary datasets and split them into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# one easy, one medium, one hard for each classification and regression\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits\n",
    "from sklearn.datasets import load_diabetes, fetch_california_housing\n",
    "\n",
    "\n",
    "ds_c_easy = load_iris()\n",
    "X, Y = ds_c_easy.data, ds_c_easy.target\n",
    "X_c_easy_train, X_c_easy_test, Y_c_easy_train, Y_c_easy_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_c_medium = load_breast_cancer()\n",
    "X, Y = ds_c_medium.data, ds_c_medium.target \n",
    "X_c_medium_train, X_c_medium_test, Y_c_medium_train, Y_c_medium_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_c_hard = load_digits()\n",
    "X, Y = ds_c_hard.data, ds_c_hard.target\n",
    "X_c_hard_train, X_c_hard_test, Y_c_hard_train, Y_c_hard_test = train_test_split(X , Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_r_easy = fetch_california_housing()\n",
    "X, Y = ds_r_easy.data, ds_r_easy.target\n",
    "X_r_easy_train, X_r_easy_test, Y_r_easy_train, Y_r_easy_test = train_test_split(X , Y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds_r_medium = load_diabetes()\n",
    "X, Y = ds_r_medium.data, ds_r_medium.target\n",
    "X_r_medium_train, X_r_medium_test, Y_r_medium_train, Y_r_medium_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at our own implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = OwnDecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "dt_classifier.fit(X_c_easy_train, Y_c_easy_train)\n",
    "Y_c_easy_pred = dt_classifier.predict(X_c_easy_test)\n",
    "\n",
    "print(classification_report(Y_c_easy_test, Y_c_easy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model achieves a perfect accuracy on the Iris dataset (This is to be expected as the dataset is very simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = SklearnDecisionTreeClassifier(max_depth=5)\n",
    "dt_classifier.fit(X_c_easy_train, Y_c_easy_train)\n",
    "Y_c_easy_pred = dt_classifier.predict(X_c_easy_test)\n",
    "\n",
    "print(classification_report(Y_c_easy_test, Y_c_easy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without much surprise, scikit-learn's Decision Tree also achieves a perfect accuracy here. Moving on the Breast Cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.92        43\n",
      "           1       0.93      0.97      0.95        71\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.94      0.93      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = OwnDecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "dt_classifier.fit(X_c_medium_train, Y_c_medium_train)\n",
    "Y_c_medium_pred = dt_classifier.predict(X_c_medium_test)\n",
    "\n",
    "print(classification_report(Y_c_medium_test, Y_c_medium_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92        43\n",
      "           1       0.94      0.96      0.95        71\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.94      0.93      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = SklearnDecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "dt_classifier.fit(X_c_medium_train, Y_c_medium_train)\n",
    "Y_c_medium_pred = dt_classifier.predict(X_c_medium_test)\n",
    "\n",
    "print(classification_report(Y_c_medium_test, Y_c_medium_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, while not completely perfect, both models still achieve a very high, similar accuracy of 94%. Finally, the digits dataset. This is particularly tricky since we are now dealing with non-binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96        33\n",
      "           1       0.21      0.32      0.25        28\n",
      "           2       0.61      0.67      0.64        33\n",
      "           3       0.82      0.82      0.82        34\n",
      "           4       0.81      0.76      0.79        46\n",
      "           5       0.71      0.32      0.44        47\n",
      "           6       0.94      0.86      0.90        35\n",
      "           7       0.92      0.71      0.80        34\n",
      "           8       0.41      0.57      0.48        30\n",
      "           9       0.56      0.70      0.62        40\n",
      "\n",
      "    accuracy                           0.67       360\n",
      "   macro avg       0.69      0.67      0.67       360\n",
      "weighted avg       0.71      0.67      0.67       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = OwnDecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "dt_classifier.fit(X_c_hard_train, Y_c_hard_train)\n",
    "Y_c_hard_pred = dt_classifier.predict(X_c_hard_test)\n",
    "\n",
    "print(classification_report(Y_c_hard_test, Y_c_hard_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.94        33\n",
      "           1       0.44      0.25      0.32        28\n",
      "           2       0.58      0.21      0.31        33\n",
      "           3       0.42      0.82      0.55        34\n",
      "           4       0.81      0.85      0.83        46\n",
      "           5       0.98      0.91      0.95        47\n",
      "           6       0.94      0.91      0.93        35\n",
      "           7       0.92      0.65      0.76        34\n",
      "           8       0.31      0.73      0.44        30\n",
      "           9       0.87      0.33      0.47        40\n",
      "\n",
      "    accuracy                           0.67       360\n",
      "   macro avg       0.73      0.65      0.65       360\n",
      "weighted avg       0.75      0.67      0.67       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = SklearnDecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "dt_classifier.fit(X_c_hard_train, Y_c_hard_train)\n",
    "Y_c_hard_pred = dt_classifier.predict(X_c_hard_test)\n",
    "\n",
    "print(classification_report(Y_c_hard_test, Y_c_hard_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately can see, that both models take a considerable hit in accuracy. Given the complexity of the dataset, this is to be expected. However, the de-facto standard implementation of a decision tree by sklearn only outperforms our own implementation by a single percentage point. This indicates that the issue doesn't lie in our implementation, but rather that we are reaching the limits of what single a decision tree can achieve on this dataset. Let's see how our ensamble methods can improve on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 9, 'min_samples_leaf': 1, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "from models.decision_tree import DecisionTreeClassifier as OwnDecisionTreeClassifier\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from models.grid_search_cv import GridSearchCV\n",
    "\n",
    "ds_c_hard = load_digits()\n",
    "X, Y = ds_c_hard.data, ds_c_hard.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X , Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'max_depth': [7, 9, 11],\n",
    "    'min_samples_split': [1, 2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "param_grid = list(ParameterGrid(params))\n",
    "\n",
    "grid_search = GridSearchCV(OwnDecisionTreeClassifier, param_grid, cv=5)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "print(grid_search.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
